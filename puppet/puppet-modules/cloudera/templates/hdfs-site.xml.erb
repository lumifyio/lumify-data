<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>dfs.replication</name>
    <value><%= @hadoop_slaves.count == 1 ? 1 : 3 %></value>
  </property>
  <property>
     <name>dfs.permissions.enabled</name>
     <value>false</value>
  </property>
  <property>
     <name>dfs.datanode.synconclose</name>
     <value>true</value>
  </property>

  <% if @hadoop_slaves.count == 1 %>
  <!-- Immediately exit safemode as soon as one DataNode checks in.
       On a multi-node cluster, these configurations must be removed.  -->
  <property>
    <name>dfs.namenode.safemode.extension</name>
    <value>0</value>
  </property>
  <property>
     <name>dfs.namenode.safemode.min.datanodes</name>
     <value>1</value>
  </property>
  <% end %>

  <property>
     <!-- specify this so that running 'hadoop namenode -format' formats the right dir -->
     <name>dfs.namenode.name.dir</name>
     <value><%= (@data_directories || '').split(',').map{|d| "file://#{d}/hdfs/name"}.join(',') %></value>
     <final>true</final>
  </property>

  <property>
     <name>dfs.datanode.data.dir</name>
     <value><%= (@data_directories || '').split(',').map{|d| "file://#{d}/hdfs/data"}.join(',') %></value>
     <final>true</final>
  </property>

  <property>
     <name>dfs.webhdfs.enabled</name>
     <value>true</value>
  </property>

  <% if @hadoop_ha_enabled == true %>
  <property>
     <name>dfs.nameservices</name>
     <value><%= @hadoop_ha_cluster_name %></value>
  </property>
  <property>
     <name>dfs.ha.namenodes.<%= @hadoop_ha_cluster_name %></name>
     <value>nn1,nn2</value>
  </property>
  <property>
     <name>dfs.namenode.rpc-address.<%= @hadoop_ha_cluster_name %>.nn1</name>
     <value><%= @hadoop_namenodes[0] %>:<%= @hadoop_namenode_rpc_port %></value>
  </property>
  <property>
     <name>dfs.namenode.rpc-address.<%= @hadoop_ha_cluster_name %>.nn2</name>
     <value><%= @hadoop_namenodes[1] %>:<%= @hadoop_namenode_rpc_port %></value>
  </property>
  <property>
     <name>dfs.namenode.http-address.<%= @hadoop_ha_cluster_name %>.nn1</name>
     <value><%= @hadoop_namenodes[0] %>:<%= @hadoop_namenode_http_port %></value>
  </property>
  <property>
     <name>dfs.namenode.http-address.<%= @hadoop_ha_cluster_name %>.nn2</name>
     <value><%= @hadoop_namenodes[1] %>:<%= @hadoop_namenode_http_port %></value>
  </property>
  <property>
     <name>dfs.namenode.shared.edits.dir</name>
     <value>qjournal://<%= @hadoop_ha_journalnodes.join(";") %>/<%= @hadoop_ha_cluster_name %></value>
  </property>
  <property>
     <name>dfs.client.failover.proxy.provider.<%= @hadoop_ha_cluster_name %></name>
     <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>

  <!-- TODO: eventually, configure real fencing -->
  <property>
     <name>dfs.ha.fencing.methods</name>
     <value>shell(/bin/true)</value>
  </property>

  <property>
     <name>dfs.journalnode.edits.dir</name>
     <value><%= @hadoop_ha_journalnode_edits_dir %></value>
  </property>

	<property>
     <name>dfs.ha.automatic-failover.enabled</name>
     <value>true</value>
  </property>

  <% else %>
  <property>
     <name>dfs.namenode.rpc-address</name>
     <value><%= @namenode_rpc_address %></value>
  </property>
  <property>
     <name>dfs.namenode.rpc-bind-host</name>
     <value><%= @namenode_rpc_bind_host %></value>
  </property>
  <% end %>

  <property>
     <name>dfs.datanode.address</name>
     <value><%= @datanode_address %></value>
  </property>
  <property>
     <name>dfs.datanode.ipc.address</name>
     <value><%= @datanode_ipc_address %></value>
  </property>
</configuration>
