<?xml version="1.0" ?><page xmlns="http://www.mediawiki.org/xml/export-0.8/" version="0.8">
    <title>Norconex HTTP Collector</title>
    <ns>0</ns>
    <id>39667222</id>
    <revision>
      <id>562726393</id>
      <parentid>562542604</parentid>
      <timestamp>2013-07-03T18:29:50Z</timestamp>
      <contributor>
        <username>Kalhomoud</username>
        <id>19177106</id>
      </contributor>
      <comment>Adding a category</comment>
      <text xml:space="preserve">{{multiple issues|
{{notability|Web|date=June 2013}}
}}

{{Infobox software

| name                   = Norconex HTTP Collector

| collapsible            = yes

| developer              = [[Norconex|Norconex Inc.]]

| status                 = Active

| latest release version = 1.0.0

| latest release date    = {{release date|2013|06|05}}

| programming language   = [[Java (programming language)|Java]]

| operating system       = [[Cross-platform]]

| genre                  = [[Web crawler|Web Crawler]]

| license                = [[GPL3|GPL]] 3.0

| website                = {{url|http://www.norconex.com/product/collector-http/}}

}}

'''Norconex HTTP Collector''' is a web spider, or crawler initially created for [[Enterprise search|Enterprise Search]] integrators and developers. It began as a closed source project developed by [[Norconex]].  It was released as [[open source]] under [[GPL3]] on June 2013.&lt;ref&gt;[https://github.com/Norconex/collectors]Source Code&lt;/ref&gt;&lt;ref&gt;[http://opensearchnews.com/2013/07/another-web-crawler-another-day/]Open Search News&lt;/ref&gt;

==Architecture==
Norconex HTTP Collector was built entirely using Java.   A single Collector installation is responsible for launching one or multiple crawler threads, each with their own configuration.

[[File:Norconex HTTP Collector Diagram.png]]

Each step is part of a crawler life-cycle is configurable and overwritable.  Developers can provide their own interface implementation for most steps undertaken by the crawler.   The default implementations provided cover a vast array of crawling use cases, and are built on stable products such as Apache Tika and Apache Derby.  The following figure is a high level representation of a URL-life-cycle from the crawler perspective.

[[File:Norconex HTTP Collector Life-Cycle.png|Norconex HTTP Collector URL-Life-Cycle]]

The Importer and Committer modules are separate GPL java libraries distributed with the Collector.

The Importer module parses incoming document from their raw form (HTML, PDF, Word, etc) to a set of extracted metadata and plain text content.  In addition, it provides interfaces to manipulate a document metadata, transform its content, or simply filter the documents based on their new format.  While the Collector is heavily dependent on the Importer module, the later can be used on its own, as a general-purpose document parser.

The committer module is responsible for directing the parsed data to a target repository of choice.  Developers are able to write custom implementations, allowing the use of Norconex HTTP Collector with any search engines or repositories.   Two committer implementations currently exists, for Apache Solr and Elastic Search.

==Minimum Requirements==
Java Standard Edition 6.0 or higher is required.  Runs on any platform supporting Java.

==Configuration==
While the Norconex HTTP Collector can be configured programmatically it also supports XML configuration files.  Apache Velocity is used to parse configuration files.   Using Velocity directives permits configuration re-use amongst different Collector installations and variables substitution.

 &lt;httpcollector id="Minimal Config HTTP Collector"&gt;
  &lt;crawlers&gt;
    &lt;crawler id="Minimal Config Wikipedia 1-page Crawl"&gt;
      &lt;!-- === Minimum required: =========================================== --&gt;
      &lt;!-- Requires at least one start URL. --&gt;
      &lt;startURLs&gt;
        &lt;url&gt;http://en.wikipedia.org/wiki/Alice%27s_Adventures_in_Wonderland&lt;/url&gt;
      &lt;/startURLs&gt;
      &lt;!-- === Minimum recommended: ======================================== --&gt;
      &lt;!-- Put a maximum depth to avoid infinite crawling (e.g. calendars). --&gt;
      &lt;maxDepth&gt;0&lt;/maxDepth&gt;
      &lt;!-- Be as nice as you can to sites you crawl. --&gt;
      &lt;delay default="5000" /&gt;
      &lt;!-- At a minimum make sure you stay on your domain. --&gt;
      &lt;httpURLFilters&gt;
        &lt;filter 
            class="com.norconex.collector.http.filter.impl.RegexURLFilter"
            onMatch="include" &gt;
          http://en\.wikipedia\.org/wiki/.*
        &lt;/filter&gt;
      &lt;/httpURLFilters&gt;
      &lt;importer&gt;
        &lt;postParseHandlers&gt;
          &lt;!-- If your target repository does not support arbitrary fields,
               make sure you only keep the fields you need. --&gt;
          &lt;tagger class="com.norconex.importer.tagger.impl.KeepOnlyTagger"
                  fields="title,keywords,description"/&gt;
        &lt;/postParseHandlers&gt;
      &lt;/importer&gt; 
      &lt;!-- Decide what to do with your files by specifying a committer. --&gt;
      &lt;committer class="com.norconex.committer.impl.FileSystemCommitter"&gt;
        &lt;directory&gt;./minimunCrawlFiles&lt;/directory&gt;
      &lt;/committer&gt;
    &lt;/crawler&gt;
  &lt;/crawlers&gt;
 &lt;/httpcollector&gt;

==See also==
[[Web crawler|Web Crawler]]

==References==
{{Reflist|2}}

==External links==
* [http://www.norconex.com/product/collector-http/ Official website]
* [https://github.com/Norconex/collectors/ Github Project Page]
* [http://www.java.com/ Oracle Java]
* [http://tika.apache.org/ Apache Tika]
* [http://db.apache.org/derby/ Apache Derby]
* [http://lucene.apache.org/solr/ Apache Solr]
* [http://www.elasticsearch.org/ Elastic Search]
* [http://velocity.apache.org/ Apache Velocity]

[[Category:Free web crawlers]]</text>
      <sha1>sz7jmph0eup8qvja1yt4s2ag936u1cz</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>