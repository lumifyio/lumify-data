<?xml version="1.0" ?><page xmlns="http://www.mediawiki.org/xml/export-0.8/" version="0.8">
    <title>Common Crawl</title>
    <ns>0</ns>
    <id>38824153</id>
    <revision>
      <id>552661886</id>
      <parentid>548898168</parentid>
      <timestamp>2013-04-29T02:54:32Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <comment>changed {{Notability}} to {{Notability|Companies}} &amp; [[WP:AWB/GF|general fixes]] using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve">{{Multiple issues|
{{refimprove|date=April 2013}}{{notability|Companies|date=March 2013}}}}
{{Infobox dot-com company
| company_name     = Common Crawl
| company_type     = [[501(c)(3)]] non-profit
| traded_as        = 
| foundation       = {{Start date|2007}}
| dissolved        =
| location         = [[San Francisco]], [[California]], USA; [[Los Angeles]], [[California]], USA
| incorporated     =
| founder          = [[Gil Elbaz]]
| chairman         = 
| president        = 
| CEO              = 
| key_people       = [[Peter Norvig]], [[Nova Spivack]], [[Carl Malamud]], [[Kurt Bollacker]]
| industry         = 
| products         = 
| services         = 
| assets           = 
| equity           = 
| owner            =
| num_employees    =
| parent           = 
| divisions        =
| subsid           = 
| company_slogan   = 
| url              = [http://commoncrawl.org/ commoncrawl.org]
| ipv6             =
| alexa            = 
| website_type     = 
| registration     = 
| num_users        =
| language         =  [[English language|English]]
| launch_date      = 
| current_status   = 
| screenshot       = 
| caption          = 
| footnotes        = 
| intl             =
}}

'''Common Crawl''' is a not for profit organization that crawls and archives the web with the intent of providing access to everyone.&lt;ref&gt;[http://commoncrawl.org/our-work/ Common Crawl Our Work]&lt;/ref&gt; The organization claims to respect nofollow and robot.txt policies.&lt;ref&gt;[https://commoncrawl.atlassian.net/wiki/display/CRWL/Frequently+Asked+Questions Common Crawl Frequently Asked Questions]&lt;/ref&gt;

Common Crawl makes available a 100 TB [[Web archiving|web archive]] of web page data from 2008 to 2012 of about 6 billion webpages.&lt;ref&gt;[https://commoncrawl.atlassian.net/wiki/display/CRWL/About+the+Data+Set Common Crawl About the Data Set]&lt;/ref&gt; Web crawl data is kept in the Amazon public datasets S3 bucket and is freely downloadable.&lt;ref&gt;[http://semanticweb.com/common-crawl-to-add-new-data-in-amazon-web-services-bucket_b27341 Common Crawl To Add New Data In Amazon Web Services Bucket]&lt;/ref&gt;&lt;ref&gt;[http://readwrite.com/2011/11/07/common_crawl_foundation_announces_5_billion_page_w New 5 Billion Page Web Index with Page Rank Now Available for Free from Common Crawl Foundation]&lt;/ref&gt; Common Crawl publishes an Open Source library for processing their data using Hadoop as well as their crawler.

==References==
{{reflist}}

==External links==
{{Commons category}}
*[http://commoncrawl.org/ Common Crawl] in California, United States
*[https://github.com/commoncrawl/ Common Crawl GitHub Repository] with the crawler, libraries and example code
*[https://groups.google.com/forum/?fromgroups#!forum/common-crawl Common Crawl Discussion Group]

[[Category:Internet companies]]
[[Category:Web archiving initiatives]]</text>
      <sha1>hew04tmtg6t76wmy57a0xok0yl5w2yw</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>