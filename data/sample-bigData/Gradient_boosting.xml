<?xml version="1.0" ?><page xmlns="http://www.mediawiki.org/xml/export-0.8/" version="0.8">
    <title>Gradient boosting</title>
    <ns>0</ns>
    <id>26649339</id>
    <revision>
      <id>561504740</id>
      <parentid>559722382</parentid>
      <timestamp>2013-06-25T11:54:55Z</timestamp>
      <contributor>
        <ip>92.79.168.32</ip>
      </contributor>
      <comment>/* Stochastic gradient boosting */ morderate -&gt; moderate</comment>
      <text xml:space="preserve">'''Gradient boosting''' is a [[machine learning]] technique for [[Regression (machine learning)|regression]] problems, which produces a prediction model in the form of an [[Ensemble learning|ensemble]] of weak prediction models, typically [[decision tree]]s. It builds the model in a stage-wise fashion like other [[Boosting (meta-algorithm)|boosting]] methods do, and it generalizes them by allowing optimization of an arbitrary [[Differentiable function|differentiable]] [[loss function]]. Gradient boosting method can also be used for [[classification (machine learning)|classification]] problems by reducing them to regression with a suitable loss function.

The method was invented by [[Jerome H. Friedman]] in 1999 and was published in a series of two papers, the first of which&lt;ref name="Friedman1999a"&gt;Friedman, J. H. "[http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf Greedy Function Approximation: A Gradient Boosting Machine.]" (February 1999)&lt;/ref&gt; introduced the method, and the second one&lt;ref name="Friedman1999b"&gt;Friedman, J. H. "[http://www-stat.stanford.edu/~jhf/ftp/stobst.pdf Stochastic Gradient Boosting.]" (March 1999)&lt;/ref&gt; described an important tweak to the algorithm, which improves its accuracy and performance.

== Gradient boosting ==
In many [[supervised learning]] problems one has an output variable ''y'' and a vector of input variables ''x'' connected together via a joint probability distribution ''P''(''x'', ''y'').&lt;!--(at least for the purposes of theoretical analysis)--&gt; Using a training set &lt;math&gt;\!(x_1, y_1), \ldots, (x_n, y_n)&lt;/math&gt; of known values of ''x'' and corresponding values of ''y'', the goal is to find an approximation &lt;math&gt;\hat{F}(x)&lt;/math&gt; to a function &lt;math&gt;\! F^*(x)&lt;/math&gt; that minimizes the expected value of some specified [[loss function]] ''L''(''y'', ''F''(''x'')):
: &lt;math&gt;F^* = \underset{F}{\operatorname{arg\,min}} E_{x,y} L(y, F(x)).&lt;/math&gt;

Gradient boosting method assumes a real-valued ''y'' and seeks an approximation &lt;math&gt;\hat{F}(x)&lt;/math&gt; in the form of a weighted sum of functions &lt;math&gt;\! h_i(x)&lt;/math&gt; from some class &lt;math&gt;\! \mathcal{H}&lt;/math&gt;, called base (or weak) learners:

: &lt;math&gt;F(x) = \sum_{i=1}^M \gamma_i h_i(x) + \mbox{const}.&lt;/math&gt;

In accordance with the [[empirical risk minimization]] principle, the method tries to find an approximation &lt;math&gt;\hat{F}(x)&lt;/math&gt; that minimizes the average value of the loss function on the training set. It does so by starting with a model, consisting of a constant function &lt;math&gt;\! F_0(x)&lt;/math&gt;, and incrementally expanding it in a [[Greedy algorithm|greedy]] fashion:

: &lt;math&gt;F_0(x) = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n L(y_i, \gamma),&lt;/math&gt;
: &lt;math&gt;F_m(x) = F_{m-1}(x) + \underset{f \in \mathcal{H}}{\operatorname{arg\,min}} \sum_{i=1}^n L(y_i, F_{m-1}(x_i) + f(x_i)),&lt;/math&gt;

where ''f'' is restricted to be a function from the class &lt;math&gt;\mathcal{H}&lt;/math&gt; of base learner functions.

However, the problem of choosing at each step the best ''f'' for an arbitrary loss function ''L'' is a hard optimization problem in general, and so we'll "cheat" by solving a much easier problem instead.

The idea is to apply a [[steepest descent]] step to this minimization problem. If we only cared about predictions at the points of the training set, and ''f'' were unrestricted, we'd update the model per the following equation, where we view ''L''(''y'', ''f'') not as a functional of ''f'', but as a function of a vector of values &lt;math&gt;\! f(x_1), \ldots, f(x_n)&lt;/math&gt;: &lt;!-- and abuse function notation blah blah blah (TODO: phrase this nicely...) --&gt;

: &lt;math&gt;F_m(x) = F_{m-1}(x) - \gamma_m \sum_{i=1}^n \nabla_f L(y_i, F_{m-1}(x_i)),&lt;/math&gt;

: &lt;math&gt;\gamma_m = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n L\left(y_i, F_{m-1}(x_i) -
          \gamma \frac{\partial L(y_i, F_{m-1}(x_i))}{\partial f(x_i)} \right).&lt;/math&gt;

But as ''f'' must come from a restricted class of functions (that's what allows us to generalize), we'll just choose the one that most closely approximates the gradient of ''L''. Having chosen ''f'', the multiplier ''Î³'' is then selected using [[line search]] just as shown in the second equation above.

In pseudocode, the generic gradient boosting method is:&lt;ref name="Friedman1999a" /&gt;&lt;ref name="hastie" /&gt;
{{framebox|blue}}
Input: training set &lt;math&gt;\! \{(x_i, y_i)\}_{i=1}^n,&lt;/math&gt; a differentiable loss function &lt;math&gt;\! L(y, F(x)),&lt;/math&gt; number of iterations &lt;math&gt;\! M.&lt;/math&gt;

Algorithm:
# Initialize model with a constant value:
#: &lt;math&gt;F_0(x) = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n L(y_i, \gamma).&lt;/math&gt;
# For ''m'' = 1 to ''M'':
## Compute so-called ''pseudo-residuals'':
##: &lt;math&gt;r_{im} = -\left[\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right]_{F(x)=F_{m-1}(x)} \quad \mbox{for } i=1,\ldots,n.&lt;/math&gt;
## Fit a base learner &lt;math&gt;\! h_m(x)&lt;/math&gt; to pseudo-residuals, i.e. train it using the training set &lt;math&gt;\{(x_i, r_{im})\}_{i=1}^n&lt;/math&gt;.
## Compute multiplier &lt;math&gt;\! \gamma_m&lt;/math&gt; by solving the following [[Line search|one-dimensional optimization]] problem:
##: &lt;math&gt;\gamma_m = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n L\left(y_i, F_{m-1}(x_i) + \gamma h_m(x_i)\right).&lt;/math&gt;
## Update the model:
##: &lt;math&gt;\! F_m(x) = F_{m-1}(x) + \gamma_m h_m(x).&lt;/math&gt;
# Output &lt;math&gt;\! F_M(x).&lt;/math&gt;
{{frame-footer}}

== Gradient tree boosting ==
Gradient boosting is typically used with [[decision tree]]s (especially [[Classification and regression tree|CART]] trees) of a fixed size as base learners. For this special case Friedman proposes a modification to gradient boosting method which improves the quality of fit of each base learner.

Generic gradient boosting at the ''m''-th step would fit a decision tree &lt;math&gt;\! h_m(x)&lt;/math&gt; to pseudo-residuals. Let &lt;math&gt;\! J&lt;/math&gt; be the number of its leaves. The tree partitions the input space into &lt;math&gt;\! J&lt;/math&gt; disjoint regions &lt;math&gt;\! R_{1m}, \ldots, R_{Jm}&lt;/math&gt; and predicts a constant value in each region. Using the [[indicator notation]], the output of &lt;math&gt;\! h_m(x)&lt;/math&gt; for input ''x'' can be written as the sum:
: &lt;math&gt;h_m(x) = \sum_{j=1}^J b_{jm} I(x \in R_{jm}),&lt;/math&gt;
where &lt;math&gt;\! b_{jm}&lt;/math&gt; is the value predicted in the region &lt;math&gt;\! R_{jm}&lt;/math&gt;.&lt;ref&gt;Note: in case of usual CART trees, the trees are fitted using least-squares loss, and so the coefficient &lt;math&gt;b_{jm}&lt;/math&gt; for the region &lt;math&gt;R_{jm}&lt;/math&gt; is equal to just the value of output variable, averaged over all training instances in &lt;math&gt;R_{jm}&lt;/math&gt;.&lt;/ref&gt;

Then the coefficients &lt;math&gt;\! b_{jm}&lt;/math&gt; are multiplied by some value &lt;math&gt;\! \gamma_m&lt;/math&gt;, chosen using line search so as to minimize the loss function, and the model is updated as follows:
: &lt;math&gt;
    F_m(x) = F_{m-1}(x) + \gamma_m h_m(x), \quad
    \gamma_m = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \gamma h_m(x_i)).
  &lt;/math&gt;

Friedman proposes to modify this algorithm so that it chooses a separate optimal value &lt;math&gt;\! \gamma_{jm}&lt;/math&gt; for each of the tree's regions, instead of a single &lt;math&gt;\! \gamma_m&lt;/math&gt; for the whole tree. He calls the modified algorithm "TreeBoost". The coefficients &lt;math&gt;\! b_{jm}&lt;/math&gt; from the tree-fitting procedure can be then simply discarded and the model update rule becomes:
: &lt;math&gt;
    F_m(x) = F_{m-1}(x) + \sum_{j=1}^J \gamma_{jm} I(x \in R_{jm}), \quad
    \gamma_{jm} = \underset{\gamma}{\operatorname{arg\,min}} \sum_{x_i \in R_{jm}} L(y_i, F_{m-1}(x_i) + \gamma h_m(x_i)).
  &lt;/math&gt;

=== Size of trees ===
&lt;math&gt;\! J&lt;/math&gt;, the number of terminal nodes in trees, is the method's parameter which can be adjusted for a data set at hand. It controls the maximum allowed level of [[Interaction (statistics)|interaction]] between variables in the model. With &lt;math&gt;\! J = 2&lt;/math&gt; ([[decision stump]]s), no interaction between variables is allowed. With &lt;math&gt;\! J = 3&lt;/math&gt; the model may include effects of the interaction between up to two variables, and so on. 

Hastie et al.&lt;ref name="hastie"&gt;{{cite book
 |last1=Hastie|first1=T.|last2=Tibshirani|first2=R.|last3=Friedman|first3=J. H.
 |year=2009
 |title=The Elements of Statistical Learning |edition=2nd
 |ISBN=0-387-84857-6
 |url=http://www-stat.stanford.edu/~tibs/ElemStatLearn/ |format= |accessdate=
 |publisher=Springer |location=New York
 |chapter=10. Boosting and Additive Trees
 |pages=337&amp;ndash;384
}}&lt;/ref&gt; comment that typically &lt;math&gt;\! 4 \leq J \leq 8&lt;/math&gt; work well for boosting and results are fairly insensitive to the choice of &lt;math&gt;\! J&lt;/math&gt; in this range, &lt;math&gt;\! J = 2&lt;/math&gt; is insufficient for many applications, and &lt;math&gt;\! J &gt; 10&lt;/math&gt; is unlikely to be required.

== Regularization ==
Fitting the training set too closely can lead to degradation of the model's generalization ability. Several so-called [[Regularization (mathematics)|regularization]] techniques reduce this [[overfitting]] effect by constraining the fitting procedure.

One natural regularization parameter is the number of gradient boosting iterations ''M'' (i.e. the number of trees in the model when the base learner is a decision tree). Increasing ''M'' reduces the error on training set, but setting it too high may lead to overfitting. An optimal value of ''M'' is often selected by monitoring prediction error on a separate validation data set. Besides controlling ''M'', several other regularization techniques are used.

=== Shrinkage ===
An important part of gradient boosting method is regularization by shrinkage which consists in modifying the update rule as follows:
: &lt;math&gt;F_m(x) = F_{m-1}(x) + \nu \cdot \gamma_m h_m(x), \quad 0 &lt; \nu \leq 1,&lt;/math&gt;
where parameter &lt;math&gt;\nu&lt;/math&gt; is called the "learning rate".

Empirically it has been found that using small learning rates (such as &lt;math&gt;\nu &lt; 0.1&lt;/math&gt;) yields dramatic improvements in model's generalization ability over gradient boosting without shrinking (&lt;math&gt;\nu = 1&lt;/math&gt;).&lt;ref name="hastie" /&gt;
However, it comes at the price of increasing computational time both during training and querying: lower learning rate requires more iterations.

=== Stochastic gradient boosting ===
Soon after the introduction of gradient boosting Friedman proposed a minor modification to the algorithm, motivated by [[Leo Breiman|Breiman]]'s [[Bootstrap aggregation|bagging]] method.&lt;ref name="Friedman1999b" /&gt; Specifically, he proposed that at each iteration of the algorithm, a base learner should be fit on a subsample of the training set drawn at random without replacement.&lt;ref&gt;Note that this is different from bagging, which samples with replacement because it uses samples of the same size as the training set.&lt;/ref&gt; Friedman observed a substantial improvement in gradient boosting's accuracy with this modification.

Subsample size is some constant fraction ''f'' of the size of the training set. When ''f'' = 1, the algorithm is deterministic and identical to the one described above. Smaller values of ''f'' introduce randomness into the algorithm and help prevent [[overfitting]], acting as a kind of [[Regularization (mathematics)|regularization]]. The algorithm also becomes faster, because regression trees have to be fit to smaller datasets at each iteration. Friedman&lt;ref name="Friedman1999b"&gt;Friedman, J. H. "[http://www-stat.stanford.edu/~jhf/ftp/stobst.pdf Stochastic Gradient Boosting.]" (March 1999)&lt;/ref&gt; obtained that &lt;math&gt;\! 0.5 \leq f \leq 0.8 &lt;/math&gt; leads to good results for small and moderate sized training sets. Therefore, ''f'' is typically set to 0.5, meaning that one half of the training set is used to build each base learner. 

Also, like in bagging, subsampling allows one to define an [[out-of-bag estimate]] of the prediction performance improvement by evaluating predictions on those observations which were not used in the building of the next base learner. Out-of-bag estimates help avoid the need for an independent validation dataset, but often underestimate actual performance improvement and the optimal number of iterations.&lt;ref name="gbm-vignette"&gt;Ridgeway, Greg (2007). [http://cran.r-project.org/web/packages/gbm/gbm.pdf Generalized Boosted Models: A guide to the gbm package.]&lt;/ref&gt;

=== Number of observations in leaves ===
Gradient tree boosting implementations often also use regularization by limiting the minimum number of observations in trees' terminal nodes (this parameter is called &lt;code&gt;n.minobsinnode&lt;/code&gt; in &lt;code&gt;gbm&lt;/code&gt; package&lt;ref name="gbm-vignette" /&gt;). It's used in the tree building process by ignoring any splits that lead to nodes containing fewer than this number of training set instances.

Imposing this limit helps to reduce variance in predictions at leaves.

== Usage ==
Recently, gradient boosting method has gained some popularity in [[learning to rank]] field. Commercial web search engines [[Yahoo]]&lt;ref&gt;Cossock, David and Zhang, Tong (2008). [http://www.stat.rutgers.edu/~tzhang/papers/it08-ranking.pdf Statistical Analysis of Bayes Optimal Subset Ranking], page 14.&lt;/ref&gt; and [[Yandex]]&lt;ref name="snezhinsk"&gt;[http://webmaster.ya.ru/replies.xml?item_no=5707&amp;ncrnd=5118 Yandex corporate blog entry about new ranking model "Snezhinsk"] (in Russian)&lt;/ref&gt; use variants of gradient boosting method in their machine-learned ranking engines.

== Names ==
The method goes by a wide variety of names. Title of the original publication&lt;ref name="Friedman1999a" /&gt; refers to it as a "Gradient Boosting Machine" (GBM). That same publication and a later one&lt;ref name="Friedman1999b" /&gt; by J. Friedman also use the names "Gradient Boost", "Stochastic Gradient Boosting" (emphasizing the random subsampling technique), "Gradient Tree Boosting" and "TreeBoost" (for specialization of the method to the case of decision trees as base learners.)

A popular open-source implementation&lt;ref name="gbm-vignette" /&gt; calls it "Generalized Boosting Model". Sometimes the method is referred to as "functional gradient boosting", "Gradient Boosted Models"&lt;!-- in Hastie et al., but this could be just a typo--&gt; and its tree version is also called "Gradient Boosted Decision Trees" (GBDT) or "Gradient Boosted Regression Trees" (GBRT). Commercial implementations from Salford Systems use the names "Multiple Additive Regression Trees" (MART) and TreeNet, both trademarked.

==Implementations==

; Open-source
*[https://sites.google.com/site/carlosbecker/resources/gradient-boosting-boosted-trees Multi-threaded MATLAB-compatible implementation of Gradient Boosting with tree weak learners]
*[http://code.google.com/p/simple-gbdt/ gradient boosted regression tree by c++, parallelization with tbb lib]
* Several open-source [[R (programming language)|R]] packages are available: &lt;code&gt;[http://cran.r-project.org/web/packages/gbm/index.html gbm]&lt;/code&gt;,&lt;ref name="gbm-vignette" /&gt; &lt;code&gt;[http://cran.r-project.org/web/packages/mboost/index.html mboost]&lt;/code&gt;
* [[Weka (machine learning)|Weka]] has an incomplete implementation in [http://weka.sourceforge.net/doc/weka/classifiers/meta/AdditiveRegression.html &lt;code&gt;weka.classifiers.meta.AdditiveRegression&lt;/code&gt;].
* [http://elf-project.sourceforge.net/ elf-project]
* [http://tmva.sourceforge.net/ TMVA: Toolkit for Multivariate Data Analysis]
* [https://sites.google.com/site/rtranking/ RT-Rank: Large Scale Multi-Threaded C++ Implementation of Gradient Boosting]
* [[OpenCV]] contains the implementation since the version 2.2&lt;ref&gt;[http://opencv.willowgarage.com/wiki/OpenCV%20Change%20Logs OpenCV change logs]&lt;/ref&gt;
* [https://mloss.org/software/view/332/ pGBRT]: C++/MPI-based parallel/distributed implementation, released September 2011
* [http://scikit-learn.org/dev/modules/ensemble.html#gradient-boosting scikit-learn]
	
; Proprietary
* [http://www.salford-systems.com/en/products/treenet TreeNet]  is a commercial implementation from Salford Systems, possibly "equipped with patent-pending extensions."
* [http://www.dtreg.com/treeboost.htm DTREG TreeBoost]
* An implementation of stochastic gradient boosting is available in [[STATISTICA]].
* [[Yahoo]] and [[Google]] have published papers describing an [[Message Passing Interface|MPI]]- and a [[MapReduce]]-based parallel implementations of gradient boosting.&lt;ref&gt;B. Panda, et al. (2009). [http://research.google.com/pubs/pub36296.html PLANET: Massively Parallel Learning of Tree Ensembles with MapReduce].&lt;/ref&gt;&lt;ref&gt;Jerry Ye, et al. (2009). [http://delivery.acm.org/10.1145/1650000/1646301/p2061-ye.pdf?key1=1646301&amp;key2=5312144621&amp;coll=GUIDE&amp;dl=ACM&amp;CFID=15151515&amp;CFTOKEN=6184618 Stochastic gradient boosted distributed decision trees].&lt;/ref&gt; However, they have not made the code publicly available.

== See also ==
* [[AdaBoost]]


== References ==
{{Reflist}}

[[Category:Decision trees]]
[[Category:Ensemble learning]]</text>
      <sha1>2qwjdumlxwg665cil3dk664ohn7p3j2</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>