<?xml version="1.0" ?><page xmlns="http://www.mediawiki.org/xml/export-0.8/" version="0.8">
    <title>Vector space model</title>
    <ns>0</ns>
    <id>20948989</id>
    <revision>
      <id>558353509</id>
      <parentid>557759659</parentid>
      <timestamp>2013-06-04T22:03:04Z</timestamp>
      <contributor>
        <ip>76.21.11.140</ip>
      </contributor>
      <comment>/* Example: tf-idf weights */</comment>
      <text xml:space="preserve">'''Vector space model''' or '''term vector model''' is an algebraic model for representing text documents (and any objects, in general) as [[vector space|vectors]] of identifiers, such as, for example, index terms. It is used in [[information filtering]], [[information retrieval]], [[index (search engine)|index]]ing and relevancy rankings.  Its first use was in the [[SMART Information Retrieval System]].

==Definitions==

Documents and queries are represented as vectors.

:&lt;math&gt;d_j = ( w_{1,j} ,w_{2,j} , \dotsc ,w_{t,j} )&lt;/math&gt;
:&lt;math&gt;q = ( w_{1,q} ,w_{2,q} , \dotsc ,w_{t,q} )&lt;/math&gt;

Each [[Dimension (vector space)|dimension]] corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is [[tf-idf]] weighting (see the example below).

The definition of ''term'' depends on the application. Typically terms are single words, [[keyword (linguistics)|keyword]]s, or longer phrases. If the words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the [[text corpus|corpus]]).

Vector operations can be used to compare documents with queries.

==Applications==

[[Image:vector space model.jpg|right|250px]]

[[Relevance (information retrieval)|Relevance]] [[ranking]]s of documents in a keyword search can be calculated, using the assumptions of [[semantic similarity|document similarities]] theory, by comparing the deviation of angles between each document vector and the original query vector where the query is represented as the same kind of vector as the documents.

In practice, it is easier to calculate the [[cosine]] of the angle between the vectors, instead of the angle itself:

:&lt;math&gt;
\cos{\theta} = \frac{\mathbf{d_2} \cdot \mathbf{q}}{\left\| \mathbf{d_2} \right\| \left \| \mathbf{q} \right\|}
&lt;/math&gt;

Where &lt;math&gt;\mathbf{d_2} \cdot \mathbf{q}&lt;/math&gt; is the intersection (i.e. the [[dot product]]) of the document (d&lt;sub&gt;2&lt;/sub&gt; in the figure to the right) and the query (q in the figure) vectors, &lt;math&gt;\left\| \mathbf{d_2} \right\|&lt;/math&gt; is the norm of vector d&lt;sub&gt;2&lt;/sub&gt;, and &lt;math&gt;\left\| \mathbf{q} \right\|&lt;/math&gt; is the norm of vector q. The [[Norm (mathematics)|norm]] of a vector is calculated as such:

:&lt;math&gt;
\left\| \mathbf{q} \right\| = \sqrt{\sum_{i=1}^n q_i^2}
&lt;/math&gt;

As all vectors under consideration by this model are elementwise nonnegative, a cosine value of zero means that the query and document vector are [[orthogonal]] and have no match (i.e. the query term does not exist in the document being considered). See [[cosine similarity]] for further information.

==Example: tf-idf weights==

In the classic vector space model proposed by [[Gerard Salton|Salton]], Wong and Yang &lt;ref&gt;[http://doi.acm.org/10.1145/361219.361220 G. Salton , A. Wong , C. S. Yang, A vector space model for automatic indexing], Communications of the ACM, v.18 n.11, p.613-620, Nov. 1975&lt;/ref&gt; the term specific weights in the document vectors are products of local and global parameters. The model is known as [[tf-idf|term frequency-inverse document frequency]] model. The weight vector for document ''d'' is &lt;math&gt;\mathbf{v}_d = [w_{1,d}, w_{2,d}, \ldots, w_{N,d}]^T&lt;/math&gt;, where

:&lt;math&gt;
w_{t,d} = \mathrm{tf}_{t,d} \cdot \log{\frac{|D|}{|\{d' \in D \, | \, t \in d'\}|}}
&lt;/math&gt;

and
* &lt;math&gt;\mathrm{tf}_{t,d}&lt;/math&gt; is term frequency of term ''t'' in document ''d'' (a local parameter)
* &lt;math&gt;\log{\frac{|D|}{|\{d' \in D \, | \, t \in d'\}|}}&lt;/math&gt; is inverse document frequency (a global parameter). &lt;math&gt;|D|&lt;/math&gt; is the total number of documents in the document set; &lt;math&gt;|\{d' \in D \, | \, t \in d'\}|&lt;/math&gt; is the number of documents containing the term ''t''.

Using the cosine the similarity between document ''d&lt;sub&gt;j&lt;/sub&gt;'' and query ''q'' can be calculated as:

:&lt;math&gt;\mathrm{sim}(d_j,q) = \frac{\mathbf{d_j} \cdot \mathbf{q}}{\left\| \mathbf{d_j} \right\| \left \| \mathbf{q} \right\|} = \frac{\sum _{i=1}^N w_{i,j}w_{i,q}}{\sqrt{\sum _{i=1}^N w_{i,j}^2}\sqrt{\sum _{i=1}^N w_{i,q}^2}}&lt;/math&gt;

==Advantages==

The vector space model has the following advantages over the [[Standard Boolean model]]:

#Simple model based on linear algebra
#Term weights not binary
#Allows computing a continuous degree of similarity between queries and documents
#Allows ranking documents according to their possible relevance
#Allows partial matching

==Limitations==

The vector space model has the following limitations:

#Long documents are poorly represented because they have poor similarity values (a small [[scalar product]] and a [[curse of dimensionality|large dimensionality]])
#Search keywords must precisely match document terms; word [[substring]]s might result in a "[[false positive]] match"
#Semantic sensitivity; documents with similar context but different term vocabulary won't be associated, resulting in a "[[false negative]] match".
#The order in which the terms appear in the document is lost in the vector space representation.
#Theoretically assumes terms are statistically independent. 
#Weighting is intuitive but not very formal. 

Many of these difficulties can, however, be overcome by the integration of various tools, including mathematical techniques such as [[singular value decomposition]] and [[lexical database]]s such as [[WordNet]].

==Models based on and extending the vector space model==

Models based on and extending the vector space model include:
* [[Generalized vector space model]]
* [[Latent semantic analysis]]
* [[Term Discrimination]]
* [[Rocchio Classification]]

==Software that implements the vector space model==

The following software packages may be of interest to those wishing to experiment with vector models and implement search services based upon them.

===Free open source software===

* [[Apache Lucene]]. Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java.
* [http://semanticvectors.googlecode.com SemanticVectors]. Semantic Vector indexes, created by applying a Random Projection algorithm (similar to [[Latent semantic analysis]]) to term-document matrices created using Apache Lucene.
* [[Gensim]] is a Python+[[NumPy]] framework for Vector Space modelling. It contains incremental (memory-efficient) algorithms for [[Tf–idf]], [[Latent Semantic Indexing]], [[Locality_sensitive_hashing#Random_projection|Random Projections]] and [[Latent Dirichlet Allocation]].
* [[Weka]]. Weka is popular data mining package for Java including WordVectors and Bag Of Words models.
* [http://codingplayground.blogspot.com/2010/03/compressed-vector-space.html Compressed vector space in C++] by Antonio Gulli
* [http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/ Text to Matrix Generator (TMG)]  MATLAB toolbox that can be used for various tasks in text mining specifically  i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. Most of TMG is written in MATLAB and parts in Perl. It contains implementations of LSI, clustered LSI, NMF and other methods.
* [http://senseclusters.sourceforge.net SenseClusters], an open source package that supports context and word clustering using Latent Semantic Analysis and word co-occurrence matrices.
* [http://code.google.com/p/airhead-research/ S-Space Package], a collection of algorithms for exploring and working with [[statistical semantics]].

==Further reading==

* [[Gerard Salton|G. Salton]], A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," ''Communications of the ACM'', vol. 18, nr. 11, pages 613–620. ''(Article in which a vector space model was presented)''
* David Dubin (2004), [http://www.ideals.uiuc.edu/bitstream/2142/1697/2/Dubin748764.pdf The Most Influential Paper Gerard Salton Never Wrote] ''(Explains the history of the Vector Space Model and the non-existence of a frequently cited publication)''
* [http://isp.imm.dtu.dk/thor/projects/multimedia/textmining/node5.html Description of the vector space model]
* [http://www.miislita.com/term-vector/term-vector-3.html Description of the classic vector space model by Dr E. Garcia]
* [http://nlp.stanford.edu/IR-book/html/htmledition/vector-space-classification-1.html Relationship of vector space search to the "k-Nearest Neighbor" search]

==See also==
*[[Bag-of-words model]]
*[[Nearest neighbor search]]
*[[Compound term processing]]
*[[Inverted index]]
*[[w-shingling]]
*[[Eigenvalues and eigenvectors]]

==References==
&lt;references/&gt;

[[Category:Vector space model|*]]</text>
      <sha1>ffngnrrdwhpiq8tleidhuhyrxtys3jr</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>